{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Types of Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize PySpark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when,avg,regexp_extract,coalesce,regexp_replace,struct,create_map,explode,to_json,from_json\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"day3\").getOrCreate()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = \"titanic.csv\"  # Replace with the actual path\n",
    "titanic_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = 'chipotle.csv' # Replace with the actual path\n",
    "chipotle_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Load the Chipotle dataset into a Spark DataFrame\n",
    "data_path = 'kalimati_tarkari_dataset.csv' # Replace with the actual path\n",
    "kalimati_df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- choice_description: string (nullable = true)\n",
      " |-- item_price: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- SN: integer (nullable = true)\n",
      " |-- Commodity: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Unit: string (nullable = true)\n",
      " |-- Minimum: double (nullable = true)\n",
      " |-- Maximum: double (nullable = true)\n",
      " |-- Average: double (nullable = true)\n",
      "\n",
      "None None None\n"
     ]
    }
   ],
   "source": [
    "print(titanic_df.printSchema(),chipotle_df.printSchema(),kalimati_df.printSchema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Spark Types:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and convert the \"Fare\" column from double to integer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- fare: integer (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the \"fare\" column from double to integer\n",
    "titanic_df = titanic_df.withColumn(\"fare\", col(\"fare\").cast(\"int\"))\n",
    "titanic_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Booleans:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and add a new column \"IsAdult\" that indicates whether a passenger is an adult (age >= 18) or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|IsAdult|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7| null|       S|   true|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|  C85|       C|   true|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7| null|       S|   true|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53| C123|       S|   true|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8| null|       S|   true|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|   8| null|       Q|  false|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|  E46|       S|   true|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21| null|       S|  false|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11| null|       S|   true|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30| null|       C|  false|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|   G6|       S|  false|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26| C103|       S|   true|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8| null|       S|   true|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31| null|       S|   true|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7| null|       S|  false|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16| null|       S|   true|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29| null|       Q|  false|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|  13| null|       S|  false|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18| null|       S|   true|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|   7| null|       C|  false|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df_age_gt_18=titanic_df.withColumn(\"IsAdult\", when(col(\"age\")>=18,True).otherwise(False))\n",
    "titanic_df_age_gt_18.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Numbers:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and calculate the average age of male and female passengers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|   Sex|          avg(age)|\n",
      "+------+------------------+\n",
      "|female|27.915708812260537|\n",
      "|  male| 30.72664459161148|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df_gender=titanic_df.groupBy(\"Sex\").agg(avg(\"age\")).alias(\"Avg\")\n",
    "titanic_df_gender.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Strings:\n",
    "\n",
    "Question: Load the \"chipotle\" dataset and find the item names containing the word \"Chicken.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
      "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
      "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
      "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
      "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
      "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
      "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
      "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 36|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 42|      20|       1|        Chicken Bowl|[Roasted Chili Co...|   $11.25 |\n",
      "| 44|      20|       1|  Chicken Salad Bowl|[Fresh Tomato Sal...|    $8.75 |\n",
      "| 45|      21|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
      "| 52|      24|       1|     Chicken Burrito|[Roasted Chili Co...|   $10.98 |\n",
      "| 63|      28|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.75 |\n",
      "| 68|      30|       1|     Chicken Burrito|[Tomatillo-Red Ch...|   $10.98 |\n",
      "| 73|      33|       1|     Chicken Burrito|[Tomatillo Red Ch...|    $8.75 |\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 16:51:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/Desktop/Spark_assignment/DAY-3/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "# chipotle_df.select(regexp_extract (col(\"item_name\"),\"Chicken\",1),\"*\").show()\n",
    "\n",
    "\n",
    "# Filter for item names containing the word \"Chicken\"\n",
    "chicken_items = chipotle_df.filter(col(\"item_name\").like(\"%Chicken%\"))\n",
    "chicken_items.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions:\n",
    "\n",
    "Question: Load the \"chipotle\" dataset and find the items with names that start with \"Ch\" followed by any character.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |\n",
      "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |\n",
      "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |\n",
      "| 10|       5|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |\n",
      "| 14|       7|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 15|       8|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |\n",
      "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |\n",
      "| 20|      10|       1| Chips and Guacamole|                null|    $4.45 |\n",
      "| 23|      12|       1|     Chicken Burrito|[[Tomatillo-Green...|   $10.98 |\n",
      "| 25|      13|       1|Chips and Fresh T...|                null|    $2.39 |\n",
      "| 26|      13|       1|        Chicken Bowl|[Roasted Chili Co...|    $8.49 |\n",
      "| 29|      15|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |\n",
      "| 30|      15|       1|Chips and Tomatil...|                null|    $2.39 |\n",
      "| 35|      18|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |\n",
      "+---+--------+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 16:52:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/Desktop/Spark_assignment/DAY-3/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter for item names that starts with \"Ch\"\n",
    "chicken_items = chipotle_df.filter(col(\"item_name\").like(\"Ch%\"))\n",
    "chicken_items.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Nulls in Data:\n",
    "\n",
    "Question: Load the \"titanic\" dataset and count the number of passengers with missing age information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passengers with missing age information: 177\n"
     ]
    }
   ],
   "source": [
    "# Count the number of passengers with missing age information\n",
    "missing_age_count = titanic_df.filter(col(\"age\").isNull()).count()\n",
    "\n",
    "# Show the count\n",
    "print(\"Number of passengers with missing age information:\", missing_age_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "Question: Utilizing the Chipotle dataset, use the coalesce function to combine the \"item_name\" and \"choice_description\" columns into a new column named \"OrderDetails.\" Display the first 5 rows of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "|_c0|order_id|quantity|           item_name|  choice_description|item_price|        OrderDetails|\n",
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "|  0|       1|       1|Chips and Fresh T...|                null|    $2.39 |Chips and Fresh T...|\n",
      "|  1|       1|       1|                Izze|        [Clementine]|    $3.39 |                Izze|\n",
      "|  2|       1|       1|    Nantucket Nectar|             [Apple]|    $3.39 |    Nantucket Nectar|\n",
      "|  3|       1|       1|Chips and Tomatil...|                null|    $2.39 |Chips and Tomatil...|\n",
      "|  4|       2|       2|        Chicken Bowl|[Tomatillo-Red Ch...|   $16.98 |        Chicken Bowl|\n",
      "|  5|       3|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $10.98 |        Chicken Bowl|\n",
      "|  6|       3|       1|       Side of Chips|                null|    $1.69 |       Side of Chips|\n",
      "|  7|       4|       1|       Steak Burrito|[Tomatillo Red Ch...|   $11.75 |       Steak Burrito|\n",
      "|  8|       4|       1|    Steak Soft Tacos|[Tomatillo Green ...|    $9.25 |    Steak Soft Tacos|\n",
      "|  9|       5|       1|       Steak Burrito|[Fresh Tomato Sal...|    $9.25 |       Steak Burrito|\n",
      "| 10|       5|       1| Chips and Guacamole|                null|    $4.45 | Chips and Guacamole|\n",
      "| 11|       6|       1|Chicken Crispy Tacos|[Roasted Chili Co...|    $8.75 |Chicken Crispy Tacos|\n",
      "| 12|       6|       1|  Chicken Soft Tacos|[Roasted Chili Co...|    $8.75 |  Chicken Soft Tacos|\n",
      "| 13|       7|       1|        Chicken Bowl|[Fresh Tomato Sal...|   $11.25 |        Chicken Bowl|\n",
      "| 14|       7|       1| Chips and Guacamole|                null|    $4.45 | Chips and Guacamole|\n",
      "| 15|       8|       1|Chips and Tomatil...|                null|    $2.39 |Chips and Tomatil...|\n",
      "| 16|       8|       1|     Chicken Burrito|[Tomatillo-Green ...|    $8.49 |     Chicken Burrito|\n",
      "| 17|       9|       1|     Chicken Burrito|[Fresh Tomato Sal...|    $8.49 |     Chicken Burrito|\n",
      "| 18|       9|       2|         Canned Soda|            [Sprite]|    $2.18 |         Canned Soda|\n",
      "| 19|      10|       1|        Chicken Bowl|[Tomatillo Red Ch...|    $8.75 |        Chicken Bowl|\n",
      "+---+--------+--------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 16:59:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, quantity, item_name, choice_description, item_price\n",
      " Schema: _c0, order_id, quantity, item_name, choice_description, item_price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ubuntu/Desktop/Spark_assignment/DAY-3/chipotle.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine the \"item_name\" and \"choice_description\" columns using coalesce\n",
    "\n",
    "combined_df = chipotle_df.withColumn(\"OrderDetails\", coalesce(col(\"item_name\"), col(\"choice_description\")))\n",
    "combined_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ifnull, nullIf, nvl, and nvl2\n",
    "\n",
    "Question: Replace the null values in the \"Age\" column of the Titanic dataset with the average age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|  71|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|   7| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|  53| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877|   8| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|  51|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909|  21| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|  11| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|  30| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|  16|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082|  31| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406|   7| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|  16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652|  29| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|  13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|  18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|   7| null|       C|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_age=titanic_df.agg(avg(col(\"age\"))).first()[0]\n",
    "# avg_age\n",
    "\n",
    "# Replace null values in the \"age\" column with the average age using ifnull\n",
    "# titanic_df_ifnull = titanic_df.withColumn(\"age\", ifnull(col(\"age\"), average_age))\n",
    "# titanic_df_ifnull.show()\n",
    "\n",
    "# Replace null values in the \"age\" column with the average age using when\n",
    "titanic_df_replace_null = titanic_df.withColumn(\"age\", when(col(\"age\").isNull(), avg_age).otherwise(col(\"age\")))\n",
    "titanic_df_replace_null.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop\n",
    "\n",
    "Question: Remove the \"Cabin\" column from the Titanic dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877|   8|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|  13|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|   7|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.drop(col(\"Cabin\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill\n",
    "\n",
    "Question: Fill the null values in the \"Age\" column of the Titanic dataset with a default age of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|  71|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|   7| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|  53| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|30.0|    0|    0|          330877|   8| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|  51|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909|  21| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|  11| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|  30| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|  16|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082|  31| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406|   7| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|  16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652|  29| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|30.0|    0|    0|          244373|  13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|  18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|30.0|    0|    0|            2649|   7| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill null values in the \"age\" column with a default age of 30 using .na.fill\n",
    "\n",
    "titanic_df.na.fill(30, subset=[\"age\"]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  replace\n",
    "\n",
    "Question: Replace the gender \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\" column of the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|Sex| Age|SibSp|Parch|          Ticket|fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  m|22.0|    1|    0|       A/5 21171|   7| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|  f|38.0|    1|    0|        PC 17599|  71|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|  f|26.0|    0|    0|STON/O2. 3101282|   7| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|  f|35.0|    1|    0|          113803|  53| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  m|35.0|    0|    0|          373450|   8| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  m|null|    0|    0|          330877|   8| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  m|54.0|    0|    0|           17463|  51|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  m| 2.0|    3|    1|          349909|  21| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|  f|27.0|    0|    2|          347742|  11| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|  f|14.0|    1|    0|          237736|  30| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|  f| 4.0|    1|    1|         PP 9549|  16|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|  f|58.0|    0|    0|          113783|  26| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  m|20.0|    0|    0|       A/5. 2151|   8| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  m|39.0|    1|    5|          347082|  31| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|  f|14.0|    0|    0|          350406|   7| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|  f|55.0|    0|    0|          248706|  16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  m| 2.0|    4|    1|          382652|  29| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  m|null|    0|    0|          244373|  13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|  f|31.0|    1|    0|          345763|  18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|  f|null|    0|    0|            2649|   7| null|       C|\n",
      "+-----------+--------+------+--------------------+---+----+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/Spark_assignment/DAY-3/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:4317: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
      "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "# Replace \"male\" with \"M\" and \"female\" with \"F\" in the \"Sex\" column using regexp_replace\n",
    "titanic_df.na.replace({\"male\":\"m\",\"female\":\"f\"},\"sex\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Working with Complex Types: Structs\n",
    "\n",
    "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceRange\" that is a struct containing \"Minimum\" and \"Maximum\" prices for each commodity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "|  PriceRange| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|\n",
      "+------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "|{35.0, 40.0}|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|\n",
      "|{26.0, 32.0}|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|\n",
      "|{20.0, 21.0}|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|\n",
      "|{15.0, 16.0}|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|\n",
      "|{28.0, 30.0}|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|\n",
      "|{30.0, 35.0}|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "| {6.0, 10.0}|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|\n",
      "|{30.0, 35.0}|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "|{35.0, 40.0}|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|\n",
      "|{25.0, 30.0}|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "|{16.0, 18.0}| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|\n",
      "|{20.0, 22.0}| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|\n",
      "|{20.0, 25.0}| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|\n",
      "|{55.0, 60.0}| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|\n",
      "|{25.0, 30.0}| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "|{60.0, 70.0}| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|\n",
      "|{14.0, 16.0}| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|\n",
      "|{15.0, 20.0}| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|\n",
      "|{30.0, 35.0}| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "|{25.0, 30.0}| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "+------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#struct containing \"Minimum\" and \"Maximum\" prices for each commodity\n",
    "kalimati_df.select(struct(\"Minimum\",\"Maximum\").alias(\"PriceRange\"),\"*\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: Arrays\n",
    "Question: Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"CommodityList\" that is an array of all the commodities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
      "| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|       CommodityList|\n",
      "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
      "|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|[Tomato Big(Nepali)]|\n",
      "|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|[Tomato Small(Loc...|\n",
      "|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|        [Potato Red]|\n",
      "|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|      [Potato White]|\n",
      "|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|[Onion Dry (Indian)]|\n",
      "|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|     [Carrot(Local)]|\n",
      "|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|    [Cabbage(Local)]|\n",
      "|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|       [Cauli Local]|\n",
      "|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|       [Raddish Red]|\n",
      "|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[Raddish White(Lo...|\n",
      "| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|      [Brinjal Long]|\n",
      "| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|     [Brinjal Round]|\n",
      "| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|     [Cow pea(Long)]|\n",
      "| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|        [Green Peas]|\n",
      "| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|[French Bean(Local)]|\n",
      "| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|    [Soyabean Green]|\n",
      "| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|      [Bitter Gourd]|\n",
      "| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|      [Bottle Gourd]|\n",
      "| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|[Pointed Gourd(Lo...|\n",
      "| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|       [Snake Gourd]|\n",
      "+---+--------------------+----------+----+-------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#new DataFrame from the Kalimati Tarkari dataset, including a new column \"CommodityList\" that is an array of all the commodities\n",
    "\n",
    "complex_array_df = kalimati_df.selectExpr(\"*\",\"array(Commodity) as CommodityList\")\n",
    "complex_array_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: explode\n",
    "\n",
    "Question: Explode the \"CommodityList\" array column from the previous step to generate a new row for each commodity in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+----+-------+-------+-------+--------------------+-------------------+\n",
      "| SN|          Commodity|      Date|Unit|Minimum|Maximum|Average|       CommodityList|           exploded|\n",
      "+---+-------------------+----------+----+-------+-------+-------+--------------------+-------------------+\n",
      "|  0| Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|[Tomato Big(Nepali)]| Tomato Big(Nepali)|\n",
      "|  1|Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|[Tomato Small(Loc...|Tomato Small(Local)|\n",
      "|  2|         Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|        [Potato Red]|         Potato Red|\n",
      "|  3|       Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|      [Potato White]|       Potato White|\n",
      "|  4| Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|[Onion Dry (Indian)]| Onion Dry (Indian)|\n",
      "+---+-------------------+----------+----+-------+-------+-------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode commoditylist array using explode method\n",
    "complex_array_df.withColumn(\"exploded\",explode(\"CommodityList\")).show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Types: Maps\n",
    "\n",
    "Question: .Create a new DataFrame from the Kalimati Tarkari dataset, including a new column \"PriceMap\" that is a map with \"Commodity\" as the key and \"Average\" price as the value\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "|            PriceMap| SN|           Commodity|      Date|Unit|Minimum|Maximum|Average|\n",
      "+--------------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "|{Tomato Big(Nepal...|  0|  Tomato Big(Nepali)|2013-06-16|  Kg|   35.0|   40.0|   37.5|\n",
      "|{Tomato Small(Loc...|  1| Tomato Small(Local)|2013-06-16|  Kg|   26.0|   32.0|   29.0|\n",
      "|{Potato Red -> 20.5}|  2|          Potato Red|2013-06-16|  Kg|   20.0|   21.0|   20.5|\n",
      "|{Potato White -> ...|  3|        Potato White|2013-06-16|  Kg|   15.0|   16.0|   15.5|\n",
      "|{Onion Dry (India...|  4|  Onion Dry (Indian)|2013-06-16|  Kg|   28.0|   30.0|   29.0|\n",
      "|{Carrot(Local) ->...|  5|       Carrot(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "|{Cabbage(Local) -...|  6|      Cabbage(Local)|2013-06-16|  Kg|    6.0|   10.0|    8.0|\n",
      "|{Cauli Local -> 3...|  7|         Cauli Local|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "|{Raddish Red -> 3...|  8|         Raddish Red|2013-06-16|  Kg|   35.0|   40.0|   37.5|\n",
      "|{Raddish White(Lo...|  9|Raddish White(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "|{Brinjal Long -> ...| 10|        Brinjal Long|2013-06-16|  Kg|   16.0|   18.0|   17.0|\n",
      "|{Brinjal Round ->...| 11|       Brinjal Round|2013-06-16|  Kg|   20.0|   22.0|   21.0|\n",
      "|{Cow pea(Long) ->...| 12|       Cow pea(Long)|2013-06-16|  Kg|   20.0|   25.0|   22.5|\n",
      "|{Green Peas -> 57.5}| 13|          Green Peas|2013-06-16|  Kg|   55.0|   60.0|   57.5|\n",
      "|{French Bean(Loca...| 14|  French Bean(Local)|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "|{Soyabean Green -...| 15|      Soyabean Green|2013-06-16|  Kg|   60.0|   70.0|   65.0|\n",
      "|{Bitter Gourd -> ...| 16|        Bitter Gourd|2013-06-16|  Kg|   14.0|   16.0|   15.0|\n",
      "|{Bottle Gourd -> ...| 17|        Bottle Gourd|2013-06-16|  Kg|   15.0|   20.0|   17.5|\n",
      "|{Pointed Gourd(Lo...| 18|Pointed Gourd(Local)|2013-06-16|  Kg|   30.0|   35.0|   32.5|\n",
      "|{Snake Gourd -> 2...| 19|         Snake Gourd|2013-06-16|  Kg|   25.0|   30.0|   27.5|\n",
      "+--------------------+---+--------------------+----------+----+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using map to map commodity as key and average as value\n",
    "kalimati_df.select(create_map(col(\"Commodity\"),col(\"Average\")).alias(\"PriceMap\"),\"*\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with JSON\n",
    "\n",
    "Question: Convert the \"kalimati_df\" DataFrame to JSON format and write it to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Invalid usage of '*' in expression 'alias'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kalimatiJSON \u001b[39m=\u001b[39m kalimati_df\u001b[39m.\u001b[39;49mselectExpr(\u001b[39m\"\u001b[39;49m\u001b[39m(*) as KalimatiJson\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mselect(to_json(col(\u001b[39m\"\u001b[39m\u001b[39mKalimatiJson\u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[1;32m      2\u001b[0m \u001b[39m# jsonKalimati.show()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m kalimatiJSON\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mjson(\u001b[39m\"\u001b[39m\u001b[39mkalimatiJSON.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Spark_assignment/DAY-3/venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3076\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(expr) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(expr[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m   3075\u001b[0m     expr \u001b[39m=\u001b[39m expr[\u001b[39m0\u001b[39m]  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 3076\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselectExpr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jseq(expr))\n\u001b[1;32m   3077\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Desktop/Spark_assignment/DAY-3/venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Spark_assignment/DAY-3/venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Invalid usage of '*' in expression 'alias'."
     ]
    }
   ],
   "source": [
    "kalimatiJSON = kalimati_df.selectExpr(\"(Sn,Commodity,date,unit,minimum,maximum,average) as KalimatiJson\").select(to_json(col(\"KalimatiJson\")))\n",
    "# jsonKalimati.show()\n",
    "kalimatiJSON.write.json(\"kalimatiJSON.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
