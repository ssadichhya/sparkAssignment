DAY-2

This README provides a brief overview of the PySpark DataFrame operations performed in the code.

## Introduction

This code demonstrates common PySpark DataFrame operations, including selecting columns, filtering rows, counting data, adding new columns, and more.

## Getting Started

Before running the code, ensure you have PySpark installed and configured in your environment. You'll also need a dataset for analysis.

## Key Operations

1. **Selecting Columns**: Showcases how to select specific columns from a DataFrame.

2. **Filtering Rows**: Filters rows based on a condition, e.g., users older than 30.

3. **Counting and Grouping**: Counts users in each occupation category.

4. **Adding a New Column**: Adds an "age_group" column based on user age.

5. **Creating DataFrames**: Creates DataFrames with provided data and schema.

6. **Adding and Renaming Columns**: Demonstrates adding and renaming columns.

7. **Filtering and Sorting**: Filters users younger than 30 and sorts by age.

8. **Repartitioning and Collecting**: Repartitions a DataFrame and collects all rows.

## Additional Questions

The code addresses additional questions, such as filtering and calculating averages, concatenating columns, and more.

